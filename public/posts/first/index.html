<!doctype html><html lang=en dir=auto><head><script src="/log/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=log/livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>First Sample | ExampleSite</title>
<meta name=keywords content><meta name=description content="Hugging face demo Pipeline for a chatbot based on my logs
get the log files colab test
Hugging face space Visit https://huggingface.co/new-space and fill in the form.
LangChain Bug log Chatbot: LangChain -> generates prompt and process -> LLM -> Generates response. In principle, for blogs and these short contests, it is sufficient to directly generate a prompt+query using LLM. Using LangChain system that is still being developed, is overly complicated."><meta name=author content="Nxxxxx"><link rel=canonical href=http://localhost:1313/log/posts/first/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/log/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z+V9+cO1A=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/log/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/log/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/log/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=http://localhost:1313/log/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=http://localhost:1313/log/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/log/posts/first/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><script>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="First Sample"><meta property="og:description" content="Hugging face demo Pipeline for a chatbot based on my logs
get the log files colab test
Hugging face space Visit https://huggingface.co/new-space and fill in the form.
LangChain Bug log Chatbot: LangChain -> generates prompt and process -> LLM -> Generates response. In principle, for blogs and these short contests, it is sufficient to directly generate a prompt+query using LLM. Using LangChain system that is still being developed, is overly complicated."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/log/posts/first/"><meta property="og:image" content="http://localhost:1313/log/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-22T15:23:51+08:00"><meta property="article:modified_time" content="2024-02-22T15:23:51+08:00"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/log/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="First Sample"><meta name=twitter:description content="Hugging face demo Pipeline for a chatbot based on my logs
get the log files colab test
Hugging face space Visit https://huggingface.co/new-space and fill in the form.
LangChain Bug log Chatbot: LangChain -> generates prompt and process -> LLM -> Generates response. In principle, for blogs and these short contests, it is sufficient to directly generate a prompt+query using LLM. Using LangChain system that is still being developed, is overly complicated."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/log/posts/"},{"@type":"ListItem","position":2,"name":"First Sample","item":"http://localhost:1313/log/posts/first/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"First Sample","name":"First Sample","description":"Hugging face demo Pipeline for a chatbot based on my logs\nget the log files colab test\nHugging face space Visit https://huggingface.co/new-space and fill in the form.\nLangChain Bug log Chatbot: LangChain -\u0026gt; generates prompt and process -\u0026gt; LLM -\u0026gt; Generates response. In principle, for blogs and these short contests, it is sufficient to directly generate a prompt+query using LLM. Using LangChain system that is still being developed, is overly complicated.","keywords":[],"articleBody":"Hugging face demo Pipeline for a chatbot based on my logs\nget the log files colab test\nHugging face space Visit https://huggingface.co/new-space and fill in the form.\nLangChain Bug log Chatbot: LangChain -\u003e generates prompt and process -\u003e LLM -\u003e Generates response. In principle, for blogs and these short contests, it is sufficient to directly generate a prompt+query using LLM. Using LangChain system that is still being developed, is overly complicated.\nfrom langchain_community import embeddings Chroma.from_documents(documents=doc_splits,collection_name=\"rag-chroma\",embedding=embeddings.ollama.OllamaEmbeddings(model='nomic-embed-text'),) This line of code can easily generate the bugs in colab and hugging face spaces\nValueError: Error raised by inference endpoint: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/embeddings (Caused by NewConnectionError(': Failed to establish a new connection: [Errno 111] Connection refused')) Possible reason: Langchain will set the base_url as ’localhost:11434’ as the server when creating embeddings.\nThe method I used: Download Llama 2 and run it locally.\nThe official LangChain documentation introduces the use of llama.cpp (https://python.langchain.com/docs/integrations/llms/llamacpp) But don’t forget to download the model. https://api.python.langchain.com/en/latest/embeddings/langchain_community.embeddings.llamacpp.LlamaCppEmbeddings.html\nThe one I use in hf is llama-2-7b.gguf.q4_0.bin (Downloaded q4_0 from https://huggingface.co/TheBloke/Llama-2-7B-GGML and then converted to gguf using convert-llama-ggml-to-gguf.py)\nIf you want to use this chatbot, you need to spend a long time before you can get feedback, so please be patient if there is no error running. References:\nllama_print_timings: load time = 4325.45 ms llama_print_timings: sample time = 0.00 ms / 1 runs ( 0.00 ms per token, inf tokens per second) llama_print_timings: prompt eval time = 5808.80 ms / 8 tokens ( 726.10 ms per token, 1.38 tokens per second) llama_print_timings: eval time = 0.00 ms / 1 runs ( 0.00 ms per token, inf tokens per second) llama_print_timings: total time = 5810.46 ms / 9 tokens WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1 llama_print_timings: load time = 4365.77 ms llama_print_timings: sample time = 163.78 ms / 256 runs ( 0.64 ms per token, 1563.04 tokens per second) llama_print_timings: prompt eval time = 171864.19 ms / 292 tokens ( 588.58 ms per token, 1.70 tokens per second) llama_print_timings: eval time = 201403.05 ms / 255 runs ( 789.82 ms per token, 1.27 tokens per second) llama_print_timings: total time = 374587.81 ms / 547 tokens Wait until I finish updating my notes on the course (TinyML and Efficient Deep Learning Computing 6.5940 • Fall • 2023, hanlab link) before making a new quantized acceleration model (if I still remember).\n","wordCount":"406","inLanguage":"en","datePublished":"2024-02-22T15:23:51+08:00","dateModified":"2024-02-22T15:23:51+08:00","author":{"@type":"Person","name":"Nxxxxx"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/log/posts/first/"},"publisher":{"@type":"Organization","name":"ExampleSite","logo":{"@type":"ImageObject","url":"http://localhost:1313/log/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/log/ accesskey=h title="Home (Alt + H)"><img src=http://localhost:1313/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=http://localhost:1313/log/categories/ title=categories><span>categories</span></a></li><li><a href=http://localhost:1313/log/tags/ title=tags><span>tags</span></a></li><li><a href=https://huggingface.co/spaces/NingHugginngFake223/chatdemo title=chatdemo.org><span>chatdemo.org</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/log/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/log/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">First Sample</h1><div class=post-meta><span title='2024-02-22 15:23:51 +0800 CST'>February 22, 2024</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;406 words&nbsp;·&nbsp;Nxxxxx</div></header><div class=post-content><h1 id=hugging-face-demo>Hugging face demo<a hidden class=anchor aria-hidden=true href=#hugging-face-demo>#</a></h1><p>Pipeline for a chatbot based on my logs</p><h2 id=get-the-log-files>get the log files<a hidden class=anchor aria-hidden=true href=#get-the-log-files>#</a></h2><p>colab test</p><h2 id=hugging-face-space>Hugging face space<a hidden class=anchor aria-hidden=true href=#hugging-face-space>#</a></h2><p>Visit <a href=https://huggingface.co/new-space>https://huggingface.co/new-space</a> and fill in the form.</p><h2 id=langchain-bug-log>LangChain Bug log<a hidden class=anchor aria-hidden=true href=#langchain-bug-log>#</a></h2><p>Chatbot: LangChain -> generates prompt and process -> LLM -> Generates response.
In principle, for blogs and these short contests, it is sufficient to directly generate a prompt+query using LLM. Using LangChain system that is still being developed, is overly complicated.</p><pre tabindex=0><code>from langchain_community import embeddings
Chroma.from_documents(documents=doc_splits,collection_name=&#34;rag-chroma&#34;,embedding=embeddings.ollama.OllamaEmbeddings(model=&#39;nomic-embed-text&#39;),)
</code></pre><p>This line of code can easily generate the bugs in colab and hugging face spaces</p><pre tabindex=0><code>ValueError: Error raised by inference endpoint: HTTPConnectionPool(host=&#39;localhost&#39;, port=11434): Max retries exceeded with url: /api/embeddings (Caused by NewConnectionError(&#39;&lt;urllib3.connection.HTTPConnection object at 0x7fd7001d4520&gt;: Failed to establish a new connection: [Errno 111] Connection refused&#39;))
</code></pre><p>Possible reason: Langchain will set the base_url as &rsquo;localhost:11434&rsquo; as the server when creating embeddings.</p><p>The method I used: Download Llama 2 and run it locally.</p><p>The official LangChain documentation introduces the use of llama.cpp (<a href=https://python.langchain.com/docs/integrations/llms/llamacpp>https://python.langchain.com/docs/integrations/llms/llamacpp</a>) But don&rsquo;t forget to download the model.
<a href=https://api.python.langchain.com/en/latest/embeddings/langchain_community.embeddings.llamacpp.LlamaCppEmbeddings.html>https://api.python.langchain.com/en/latest/embeddings/langchain_community.embeddings.llamacpp.LlamaCppEmbeddings.html</a></p><p>The one I use in hf is llama-2-7b.gguf.q4_0.bin (Downloaded q4_0 from <a href=https://huggingface.co/TheBloke/Llama-2-7B-GGML>https://huggingface.co/TheBloke/Llama-2-7B-GGML</a> and then converted to gguf using convert-llama-ggml-to-gguf.py)</p><p>If you want to use this chatbot, you need to spend a long time before you can get feedback, so please be patient if there is no error running.
References:</p><pre tabindex=0><code>llama_print_timings:        load time =    4325.45 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    5808.80 ms /     8 tokens (  726.10 ms per token,     1.38 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    5810.46 ms /     9 tokens
WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1

llama_print_timings:        load time =    4365.77 ms
llama_print_timings:      sample time =     163.78 ms /   256 runs   (    0.64 ms per token,  1563.04 tokens per second)
llama_print_timings: prompt eval time =  171864.19 ms /   292 tokens (  588.58 ms per token,     1.70 tokens per second)
llama_print_timings:        eval time =  201403.05 ms /   255 runs   (  789.82 ms per token,     1.27 tokens per second)
llama_print_timings:       total time =  374587.81 ms /   547 tokens
</code></pre><p>Wait until I finish updating my notes on the course (TinyML and Efficient Deep Learning Computing 6.5940 • Fall • 2023, <a href=https://hanlab.mit.edu/courses/2023-fall-65940>hanlab link</a>) before making a new quantized acceleration model (if I still remember).</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=http://localhost:1313/log/posts/small-dit/><span class=title>« Prev</span><br><span>Small Dit</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/log/>ExampleSite</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>